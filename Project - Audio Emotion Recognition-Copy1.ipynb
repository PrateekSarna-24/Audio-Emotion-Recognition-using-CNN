{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb80210",
   "metadata": {},
   "source": [
    "#  Audio Emotion Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d916774",
   "metadata": {},
   "source": [
    "> Audio emotion recognition is a field of artificial intelligence and signal processing that focuses on the automatic detection and analysis of human emotions from audio data, such as speech or music"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f74f47",
   "metadata": {},
   "source": [
    "### This Project Has Been Divided Into 9 Parts\n",
    "- Understanding \"Audio\" Data\n",
    "- Creating Metadata\n",
    "- Extracting Data\n",
    "- Exploring Data\n",
    "- Mel-frequency cepstral coefficients (MFCCs)\n",
    "- Processing Data for Deep Learning\n",
    "- Setting up Deep Learning Model\n",
    "- Training and Testing The Model\n",
    "- Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88bfc00",
   "metadata": {},
   "source": [
    "## 1. Understanding \"Audio\" Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa57928",
   "metadata": {},
   "source": [
    "> \"Audio\" refers to sound, particularly in the form of vibrations or waves that travel through a medium, such as air, water, or solid objects\n",
    "#### How Sound is Represented?\n",
    "There are severel ways in which we can represent a sound wave. But the important ones are: \n",
    "- Time Domain\n",
    ">We usually represent the sound in the form of the waveform. The plot is made w.r.t \"Time\" & \"Amplitude\"\n",
    "<img src = \"waveform_img.png\" style = \"width:400px;height:200px\"/>\n",
    "- Frequency Domain\n",
    "> Here we represent the sound in the form of Spectogram. The plot is made w.r.t \"Frequency\" & \"Amplitude\" & \"Phase\"\n",
    "<img src = \"spectogram_img.png\" style = \"width:400px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44060ee8",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b193b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "from librosa.display import waveshow\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf7fe0f",
   "metadata": {},
   "source": [
    "## 2. Creating Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca258f94",
   "metadata": {},
   "source": [
    "#### Creating Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426236fd",
   "metadata": {},
   "source": [
    "The following recursive function traverses through each folder and file, and returns audio files in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7eaf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(path, list_of_files, class_) :\n",
    "    \n",
    "    ## list of all content in the folder\n",
    "    file_folder = os.listdir(path)\n",
    "    temp_class = class_\n",
    "    \n",
    "    ## travering each content\n",
    "    for content in file_folder :\n",
    "        ## if file then append\n",
    "        if '.wav' in content :\n",
    "            list_of_files.append((path + '/' + content, class_))\n",
    "        ## if folder, then make a recursive call\n",
    "        else :\n",
    "            temp_class += 1\n",
    "            get_metadata(path + '/' + content, list_of_files, temp_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a19be",
   "metadata": {},
   "source": [
    "#### Initializing Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4994138",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'TESS Toronto emotional speech set data'\n",
    "metadata = []\n",
    "class_initial = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7466708c",
   "metadata": {},
   "source": [
    "#### Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae8af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metadata(path, metadata, class_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5fcac",
   "metadata": {},
   "source": [
    "#### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aef949",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.DataFrame(metadata, columns = ['File_name', 'class'])\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01fce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding age factor\n",
    "age_factor = list()\n",
    "for class_ in metadata['class'] :\n",
    "    if class_ < 7 :\n",
    "        age_factor.append('young')\n",
    "    else :\n",
    "        age_factor.append('old')\n",
    "metadata['Age_Factor'] = age_factor\n",
    "\n",
    "df = metadata.copy()\n",
    "\n",
    "def change_class(num_class) :\n",
    "    if (num_class >= 7) :\n",
    "        num_class -= 7\n",
    "    return num_class\n",
    "\n",
    "new_class = df['class'].apply(change_class)\n",
    "\n",
    "df['class'] = new_class\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7216239a",
   "metadata": {},
   "source": [
    "## 3. Extracting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd0fb44",
   "metadata": {},
   "source": [
    "The Following Function loads the audio files and return the Audio Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef77f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_audio(files) :\n",
    "    \n",
    "    ## audio signals is the array of all the loaded audio files.\n",
    "    audio_signals = []\n",
    "    \n",
    "    for file_path in files :\n",
    "        ## the load() returns the array of signal and sample rate w.r.t time, for any audio file.\n",
    "        audio, sample_rate = librosa.load(file_path)\n",
    "        audio_signals.append(audio)\n",
    "    \n",
    "    return audio_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_signals = return_audio(np.array(df['File_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58de43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining constant sample rate\n",
    "sample_rate = 22050"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d29ac",
   "metadata": {},
   "source": [
    "## 4. Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9120c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_audio = audio_signals[int(np.random.random() * 100)]\n",
    "list(sample_audio[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c2120",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,2))\n",
    "waveshow(sample_audio, sr = sample_rate)\n",
    "Audio(data = sample_audio, rate = sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f0e4ce",
   "metadata": {},
   "source": [
    "- The above signal is of TIME DOMAIN\n",
    "> The follwing audio file is converted into an array of signals.\n",
    "- Note: The Audio Signals are represented in the form of array of amplitude in each time instance\n",
    "<img src = \"waveform load.png\" style = \"width:400px;height:200px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d96ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Class_ditribution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a8bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = df['class'].value_counts().to_dict()\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d1f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Pleasant Surprise', 'Sad']\n",
    "count = list(class_distribution.values())\n",
    "\n",
    "plt.bar(emotions, count, color = 'purple')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec19b7",
   "metadata": {},
   "source": [
    "## 5. Mel-frequency cepstral coefficients (MFCCs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4381533",
   "metadata": {},
   "source": [
    "\n",
    "Mel-Frequency Cepstral Coefficients (MFCCs) are a crucial feature extraction technique widely used in the field of audio signal processing and speech recognition. They are particularly important due to their effectiveness in capturing essential patterns and characteristics in audio signals, especially for speech and audio analysis tasks. Here's an explanation of their importance and how they capture patterns in audio\n",
    "\n",
    "MFCCs are needed to Capture Patterns in Audio:\n",
    "\n",
    "- MFCCs capture patterns in audio by breaking down the audio signal into frames (typically around 20-30 milliseconds each).\n",
    "\n",
    "- For each frame, a Fourier Transform is applied to compute the power spectrum of the signal.\n",
    "\n",
    "- The power spectrum is then filtered through a bank of Mel filters, which approximate human auditory perception.\n",
    "\n",
    "- After filtering, the logarithm of the filter bank outputs is taken, followed by a Discrete Cosine Transform (DCT) to obtain the MFCC coefficients.\n",
    "\n",
    "- These coefficients represent the audio signal's spectral content for each frame.\n",
    "\n",
    "<img src = \"mfccs_img.png\" style = \"width:600;height:400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe653f7",
   "metadata": {},
   "source": [
    "#### Extracting MFCCs from Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e241c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_MFCCs(audio_signal, sample_rate) :\n",
    "    mfccs = (librosa.feature.mfcc(y = audio_signal, sr = sample_rate, n_mfcc = 13)).T\n",
    "    mfccs = np.mean(mfccs, axis = 0)\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a6dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 22050\n",
    "mfccs = list()\n",
    "for audio in audio_signals :\n",
    "    mfccs.append(extract_MFCCs(audio , sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bba601",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = pd.DataFrame(mfccs)\n",
    "mfccs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f560719",
   "metadata": {},
   "source": [
    "## 6. Processing Data for Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06debe2",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92971810",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = mfccs.values\n",
    "target = metadata['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d2dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(feature_data, target, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83dc2ef",
   "metadata": {},
   "source": [
    "#### Checking Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f1df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b1fb05",
   "metadata": {},
   "source": [
    "#### Converting Data w.r.t Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ac47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_reshaped = x_train.reshape(x_train.shape[0], 13, 1)\n",
    "y_train_reshaped = to_categorical(y_train, num_classes=len(set(target)), dtype='int')\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], 13, 1)\n",
    "y_test_reshaped = to_categorical(y_test, num_classes=len(set(target)), dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb320d7a",
   "metadata": {},
   "source": [
    "## 7. Setting up Model for Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9635adb9",
   "metadata": {},
   "source": [
    "#### The Model that will be used will be a Sequentioal Convolutional Neural Networks\n",
    "<img src = \"CNN_img.jpg\"/>\n",
    "\n",
    "#### Why CNN?\n",
    "Convolutional Neural Networks (CNNs) are primarily associated with image processing, but they can also be adapted for audio classification tasks, such as speech recognition, music genre classification, or environmental sound classification. To apply CNNs to audio data, we can use a spectrogram representation and follow a similar architecture as in image-based CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d03b7d2",
   "metadata": {},
   "source": [
    "#### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef66e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Audio_Classification :\n",
    "    \n",
    "    def __init__(self) :\n",
    "        self.model = Sequential()\n",
    "        input_shape = (13, 1)\n",
    "        self.model.add(Conv1D(32, kernel_size=3, activation='selu', input_shape=input_shape))\n",
    "        self.model.add(MaxPooling1D(pool_size=2))\n",
    "        self.model.add(Conv1D(64, kernel_size=3, activation='selu'))\n",
    "        self.model.add(MaxPooling1D(pool_size=2))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(128, activation='selu'))\n",
    "        self.model.add(Dense(14, activation='softmax'))\n",
    "        self.model.compile(loss = 'CategoricalCrossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    def fit(self, x, y, epochs, validation) :\n",
    "        self.model.fit(x, y, epochs = epochs, validation_data = validation)\n",
    "\n",
    "    def predict(self, x) :\n",
    "        y_pred = self.model.predict(x)\n",
    "        for i in range(len(y_pred)) :\n",
    "            y_pred[i] = np.argmax(y_pred[i])\n",
    "        y_pred = np.array(y_pred[:, 0], dtype = int)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a29d86d",
   "metadata": {},
   "source": [
    "## 8. Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88b4ce",
   "metadata": {},
   "source": [
    "#### Creating Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6098a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Audio_Classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc6df3",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc41fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(x_train_reshaped, y_train_reshaped, epochs = 120, validation = (x_test_reshaped, y_test_reshaped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29d444",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4397dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test_reshaped)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a72a103",
   "metadata": {},
   "source": [
    "## 9. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c312ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy : \",accuracy_score(y_test, y_pred) * 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d05b565",
   "metadata": {},
   "source": [
    "### Submitted By - Prateek Sarna & Ayushi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
